{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0541d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6eace03",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db03ae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Set device and instantiate model once\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.set_float32_matmul_precision(\"high\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae4b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1ebdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f167b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78366246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bf4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd5e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c242374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    ###Input batch:\n",
    " ###tensor([[6109, 3626, 6100,  345],\n",
    "        ##[6109, 1110, 6622,  257]])\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c4c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
    "## Model will be instantiated later on the correct device after defining `device`\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b826e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889ce3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f12550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.983052677578396\n",
      "Validation loss: 11.000890731811523\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Initial evaluation before training\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f8cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    # model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338af86",
   "metadata": {},
   "source": [
    "<div>\n",
    "    Training section\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "# model already created and moved to device earlier\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e13536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673dbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc20426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " The verdict is lean sun on a lump of the room, or him on tone on such\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The verdict is\", tokenizer).to(device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86e2c2",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h1>LOADING WEIGHTS FROM OPEN AI</h1>\n",
    "<p>\n",
    "So the thing is open ai used Tensorflow for their shi and we are using torch here.. soo gotta make a jum to tensor flow now\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8067c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow>=2.15.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: tqdm>=4.66 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (2.3.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorflow>=2.15.0) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow>=2.15.0) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow>=2.15.0) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow>=2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow>=2.15.0) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from tqdm>=4.66) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.15.0) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from keras>=3.10.0->tensorflow>=2.15.0) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from keras>=3.10.0->tensorflow>=2.15.0) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from keras>=3.10.0->tensorflow>=2.15.0) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.15.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow>=2.15.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow>=2.15.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\onedrive\\desktop\\python_projects\\llm_from_scratch\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.15.0) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"tensorflow>=2.15.0\" \"tqdm>=4.66\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c91201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tqdm version:\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download3 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d36362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Python_projects\\llm_from_scratch\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Python_projects\\llm_from_scratch\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Python_projects\\llm_from_scratch\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Python_projects\\llm_from_scratch\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Python_projects\\llm_from_scratch\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Python_projects\\llm_from_scratch\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Python_projects\\llm_from_scratch\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e7e398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  \n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "puck = GPTModel(NEW_CONFIG)\n",
    "# gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e59a5c",
   "metadata": {},
   "source": [
    "<h2>ASSIGNING THE WEIGHTS</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0574e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right): # Assigning the weight when they have the same dims\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4aa6601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95be9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(puck, params)\n",
    "puck.to(device);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76877f42",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    GENERATING THE FINAL OUTPUT\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0b2074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " uranimum tastes like iced coffee, which is basically a type of sweetener for coffee. People usually love the flavor of lemon, that the citrus\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"uranimum tastes like \"\n",
    "\n",
    "token_ids = generate(\n",
    "    model=puck,\n",
    "    idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
