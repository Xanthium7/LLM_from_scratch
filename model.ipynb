{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0541d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6eace03",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db03ae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Set device and instantiate model once\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.set_float32_matmul_precision(\"high\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae4b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d1ebdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f167b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78366246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9bf4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fd5e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c242374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    ###Input batch:\n",
    " ###tensor([[6109, 3626, 6100,  345],\n",
    "        ##[6109, 1110, 6622,  257]])\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3c4c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
    "## Model will be instantiated later on the correct device after defining `device`\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b826e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "889ce3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52f12550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.2484870221879747\n",
      "Validation loss: 6.356018543243408\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Initial evaluation before training\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5f8cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    # model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338af86",
   "metadata": {},
   "source": [
    "<div>\n",
    "    Training section\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b2e1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.314, Val loss 7.389\n",
      "Ep 1 (Step 000005): Train loss 0.303, Val loss 7.153\n",
      "Ep 1 (Step 000005): Train loss 0.303, Val loss 7.153\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of\n",
      "Ep 2 (Step 000010): Train loss 0.197, Val loss 7.155\n",
      "Ep 2 (Step 000010): Train loss 0.197, Val loss 7.155\n",
      "Ep 2 (Step 000015): Train loss 0.136, Val loss 7.264\n",
      "Ep 2 (Step 000015): Train loss 0.136, Val loss 7.264\n",
      "Every effort moves you?\"  \"I the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the sketch, I had been--oh, I was prince\n",
      "Every effort moves you?\"  \"I the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the sketch, I had been--oh, I was prince\n",
      "Ep 3 (Step 000020): Train loss 0.123, Val loss 7.263\n",
      "Ep 3 (Step 000020): Train loss 0.123, Val loss 7.263\n",
      "Ep 3 (Step 000025): Train loss 0.126, Val loss 7.361\n",
      "Ep 3 (Step 000025): Train loss 0.126, Val loss 7.361\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 4 (Step 000030): Train loss 0.094, Val loss 7.529\n",
      "Ep 4 (Step 000030): Train loss 0.094, Val loss 7.529\n",
      "Ep 4 (Step 000035): Train loss 0.064, Val loss 7.544\n",
      "Ep 4 (Step 000035): Train loss 0.064, Val loss 7.544\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 5 (Step 000040): Train loss 0.057, Val loss 7.648\n",
      "Ep 5 (Step 000040): Train loss 0.057, Val loss 7.648\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 6 (Step 000045): Train loss 0.062, Val loss 7.673\n",
      "Ep 6 (Step 000045): Train loss 0.062, Val loss 7.673\n",
      "Ep 6 (Step 000050): Train loss 0.056, Val loss 7.677\n",
      "Ep 6 (Step 000050): Train loss 0.056, Val loss 7.677\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 7 (Step 000055): Train loss 0.053, Val loss 7.713\n",
      "Ep 7 (Step 000055): Train loss 0.053, Val loss 7.713\n",
      "Ep 7 (Step 000060): Train loss 0.038, Val loss 7.819\n",
      "Ep 7 (Step 000060): Train loss 0.038, Val loss 7.819\n",
      "Every effort moves you?\" \"Has--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's \"There were days, poor\n",
      "Every effort moves you?\" \"Has--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's \"There were days, poor\n",
      "Ep 8 (Step 000065): Train loss 0.030, Val loss 7.785\n",
      "Ep 8 (Step 000065): Train loss 0.030, Val loss 7.785\n",
      "Ep 8 (Step 000070): Train loss 0.052, Val loss 7.666\n",
      "Ep 8 (Step 000070): Train loss 0.052, Val loss 7.666\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey, and were amusing himself by holding\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey, and were amusing himself by holding\n",
      "Ep 9 (Step 000075): Train loss 0.056, Val loss 7.815\n",
      "Ep 9 (Step 000075): Train loss 0.056, Val loss 7.815\n",
      "Ep 9 (Step 000080): Train loss 0.034, Val loss 7.928\n",
      "Ep 9 (Step 000080): Train loss 0.034, Val loss 7.928\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.027, Val loss 7.823\n",
      "Ep 10 (Step 000085): Train loss 0.027, Val loss 7.823\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 1.31 minutes.\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 1.31 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "# model already created and moved to device earlier\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0286483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3e13536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "673dbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afc20426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " The verdict is to have terrace.\n",
      "\n",
      "I glanced out were arm-chairs forward\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The verdict is\", tokenizer).to(device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
